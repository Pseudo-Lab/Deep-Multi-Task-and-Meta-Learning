
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Lect 10 Model-Based RL &#8212; 멀티태스크/메타러닝 초읽기</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="prev" title="CS330 Lec9. Reinforcemeht Learning : A Primer, Multi-Task, Goal-Conditioned" href="../Ch9%20RL%20-%20A%20Primer%2C%20Multi-Task%2C%20Goal-Conditioned.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../../_static/PseudoLab_logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">멀티태스크/메타러닝 초읽기</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../intro.html">
                    인트로
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Deep Multi-Task &amp; Meta Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Ch9%20RL%20-%20A%20Primer%2C%20Multi-Task%2C%20Goal-Conditioned.html">
   CS330 Lec9. Reinforcemeht Learning : A Primer, Multi-Task, Goal-Conditioned
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Lect 10 Model-Based RL
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/executablebooks/Deep-Multi-Task-and-Meta-Learning"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/executablebooks/Deep-Multi-Task-and-Meta-Learning/issues/new?title=Issue%20on%20page%20%2F_posts/multitask-meta-learning/Ch9 RL - Model-Based Reinforcement Learning/index.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../../_sources/_posts/multitask-meta-learning/Ch9 RL - Model-Based Reinforcement Learning/index.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   개요
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   문제점
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multi-task-rl">
   Multi-task RL
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#latent-representation">
   Latent Representation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   참고 자료
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Lect 10 Model-Based RL</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   개요
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   문제점
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multi-task-rl">
   Multi-task RL
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#latent-representation">
   Latent Representation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   참고 자료
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="lect-10-model-based-rl">
<h1>Lect 10 Model-Based RL<a class="headerlink" href="#lect-10-model-based-rl" title="Permalink to this headline">#</a></h1>
<p>저자 : 이홍규</p>
<section id="id1">
<h2>개요<a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Model-based RL의 주된 모티베이션은 <strong>충분한 양의 데이터 확보</strong>입니다.</p></li>
<li><p>실제 시스템을 동작함해서 얻은 데이터는 품질은 완벽하지만, 그 양이 모델을 충분히 generalization하기에는 부족할 수밖에 없습니다.</p></li>
</ul>
<div class="important admonition">
<p class="admonition-title">Example</p>
<p>예를 들어 참다랑어 스시를 만드는 로봇을 구현한다고 가정합시다. 고추냉이가 너무 들어가서 먹으면서 눈물을 쏟는 스시도 있을 것이고, 밥이 너무 많이 들어가고 참다랑어는 눈꼽만큼 올려져 있는 스시도 있을 것입니다. 모든 형태의 스시(state)를 실제 로봇을 동작해서 표현하려고 한다면, 그건 로봇도 힘들고, 수 많은 참다랑어가 소비되어야 합니다.</p>
</div>
<ul class="simple">
<li><p>따라서 <mark>실제 환경의 transition dynamic ( <span class="math notranslate nohighlight">\(p(s'|s,a)\)</span> ) 을 모델 ( <span class="math notranslate nohighlight">\(f_\phi(s,a)\)</span> )을 통해 시뮬레이션</mark>하고, 이를 통해 데이터의 부족을 해결해보자는 접근방법이 <strong>Model-based RL</strong>이라고 할 수 있습니다.</p></li>
<li><p>다이나믹 모델은 stochastic하게 구현될 수도 deterministic하게 구현될 수도 있습니다.</p></li>
<li><p>모델은 일반적으로 아래와 같이 일반적인 supervised learning의 형태로 fitting됩니다.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathcal D = \{(s_i, a_i, s'_i)|i=1,~\cdots,~N~\}\\
\min_\phi \sum_i ||f_\phi(s_i, a_i) - s_i'||^2
\end{split}\]</div>
<ul class="simple">
<li><p>여기서 모델은 현재 state와 action을 전달받고 다음 state의 distribution을 예측하도록 피팅됩니다.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
f_\phi(s, a) = p(s'|s,a)
\]</div>
</section>
<section id="id2">
<h2>문제점<a class="headerlink" href="#id2" title="Permalink to this headline">#</a></h2>
<p>먼저 Model-Based RL은 다음과 같이 동작합니다.</p>
<div class="note admonition">
<p class="admonition-title">Peudo-Code</p>
<ol class="simple">
<li><p>어떠한 폴리시(e.g. random policy)를 통해 실제 환경으로부터 데이터를 수집합니다. <span class="math notranslate nohighlight">\(\mathcal D = \{(o_i, a_i, o'_i)\}\)</span></p></li>
<li><p>해당 데이터를 통해 모델을 피팅합니다. <span class="math notranslate nohighlight">\(s' = f_\phi(s,a)\)</span>.</p></li>
<li><p>해당 모델을 통해 <span class="math notranslate nohighlight">\(f_\phi(s, a)\)</span>, 최적의 액션 시퀀스를 산출합니다.</p></li>
</ol>
</div>
<p><img alt="image-20230107040313664" src="../../../_images/image-20230107040313664.png" /></p>
<p><a class="reference external" href="https://youtu.be/LGhVQ3NB9h4?list=PLoROMvodv4rOxuwpC_raecBCd5Jf54lEa&amp;t=1749">source</a></p>
<ul class="simple">
<li><p>하지만 아무리 실제환경을 잘 피팅했다고 해도 <mark>모델에는 어느 정도의 오차가 존재합니다. </mark></p></li>
<li><p>그리고 액션이 길어질수록 이 에러들이 쌓이면서 나중에는 무시하지 못할 수준이 될 수 있습니다.</p></li>
<li><p>이와 같은 문제의 경우, 모델이 생성한 데이터를 다시 모델 피팅에 사용함으로써 어느 정도 완화할 수 있습니다.</p></li>
<li><p>해당 알고리즘은 <strong>MPC</strong>(model-predictive control)라고 불립니다.</p></li>
</ul>
<p><img alt="image-20230107043633901" src="../../../_images/image-20230107043633901.png" /></p>
<p><a class="reference external" href="https://youtu.be/LGhVQ3NB9h4?list=PLoROMvodv4rOxuwpC_raecBCd5Jf54lEa&amp;t=1913">source</a></p>
<div class="note admonition">
<p class="admonition-title">Peudo-Code</p>
<ol class="simple">
<li><p>어떠한 폴리시(e.g. random policy)를 통해 실제 환경으로부터 데이터를 수집합니다. <span class="math notranslate nohighlight">\(\mathcal D = \{(o_i, a_i, o'_i)\}\)</span></p></li>
<li><p>해당 데이터를 통해 모델을 피팅합니다. <span class="math notranslate nohighlight">\(s' = f_\phi(s,a)\)</span>.</p></li>
<li><p>해당 모델을 통해 <span class="math notranslate nohighlight">\(f_\phi(s, a)\)</span>, 최적의 액션 시퀀스를 산출합니다.</p></li>
<li><p>산출된 최적의 액션 시퀀스에서 첫번째 액션만을 실행하고 나머지는 버립니다.</p></li>
<li><p>액션 실행 결과로 얻은 데이터 <span class="math notranslate nohighlight">\((s,a,s')\)</span>를 데이터셋에 추가합니다.</p></li>
<li><p>2번으로 돌아갑니다.</p></li>
</ol>
</div>
</section>
<section id="multi-task-rl">
<h2>Multi-task RL<a class="headerlink" href="#multi-task-rl" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>강화학습 태스크는 다음과 같이 정의됩니다.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\underbrace{\mathcal T_i}_\text{task} 
\triangleq \{\mathcal S_i,~
\mathcal A_i,~
p_i(s_1),~
p_i(s'|s,a),~
r_i(s,a)\}
\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal S_i\)</span> : state space</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal A_i\)</span> : action space</p></li>
<li><p><span class="math notranslate nohighlight">\(p_i(s_1)\)</span> : distribution of initial states</p></li>
<li><p><span class="math notranslate nohighlight">\(p_i(s'|s,a)\)</span> : transition dynamics or transition probability</p></li>
<li><p><span class="math notranslate nohighlight">\(r_i(s,a)\)</span> : reward</p></li>
<li><p>하지만 많은 경우에서, 오직 <mark><strong>리워드</strong>만이 태스크를 구분하는 요소</mark>가 되곤 합니다.</p></li>
<li><p>예를 들어 다양한 태스크를 수행하는 로봇을 만든다고 했을 때, 해당 로봇이 가질 수 있는 State와 Action은 태스크와 무관하게 동일할 것입니다.</p></li>
<li><p>하지만 해당 로봇은 태스크에 따라 리워드는 당연히 다를 것입니다.</p></li>
</ul>
<div class="important admonition">
<p class="admonition-title">Example</p>
<p>예를 들어 스테이크를 굽는 로봇을 구현한다고 가정합시다. 스테이크가 완전히 속까지 익을 정도로 구어졌다면, 레어로 굽는 태스크에서는 리워드가 마이너스 혹은 0이겠지만, 웰던으로 굽는 태스크에서는 좋은 리워드를 받을 것입니다.</p>
</div>
<p><img alt="image-20230109110949479" src="../../../_images/image-20230109110949479.png" /></p>
<p><a class="reference external" href="https://youtu.be/LGhVQ3NB9h4?list=PLoROMvodv4rOxuwpC_raecBCd5Jf54lEa&amp;t=2006">source</a></p>
<ul class="simple">
<li><p>각 태스크별로 reward function의 형태를 알고 있는 경우, 해당 reward function에 기반해, 해당 태스크에 맞게, 액션을 플래닝하면 될 것입니다.</p></li>
<li><p>하지만 태스크에 따른 reward function의 형태를 모를 경우, 우리는 태스크 별 reward function을 추정해야 할 것입니다.</p></li>
<li><p>이 때 앞서 배웠던 Multi-task Learning, 그리고 Meta Learning 방법론이 사용될 수 있습니다.</p></li>
<li><p>위 이미지에 Meta Learning을 통해 reward function을 학습시키는 어프로치가 소개되어 있는데, 2개의 Positive example만 주어진 채로, (state, action) 튜플 4개에 대해 binary한 reward를 추정하도록 훈련시키고 있습니다.</p></li>
</ul>
</section>
<section id="latent-representation">
<h2>Latent Representation<a class="headerlink" href="#latent-representation" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Model-based RL에서 자주 사용되는 어프로치 중에 하나는 observation을 상대적으로 더 낮은 차원의 latent space로 임베딩하는 것입니다.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
s = g(o)
\]</div>
<ul class="simple">
<li><p>알고리즘을 살펴보면 다음과 같습니다.</p></li>
</ul>
<div class="note admonition">
<p class="admonition-title">Peudo-Code</p>
<ol class="simple">
<li><p>어떠한 폴리시(e.g. random policy)를 통해 실제 환경으로부터 데이터를 모읍니다. <span class="math notranslate nohighlight">\(\mathcal D = \{(o_i, a_i, o'_i)\}\)</span></p></li>
<li><p>인코더(<span class="math notranslate nohighlight">\(g\)</span>)를 통해 observation들을 임베딩하고 — <span class="math notranslate nohighlight">\(s = g(o)\)</span> — 임베딩된 state들을 기반으로 모델을 피팅합니다. — <span class="math notranslate nohighlight">\(s' = f_\phi(s,a)\)</span>.</p></li>
<li><p>모델(<span class="math notranslate nohighlight">\(f_\phi(s, a)\)</span>)을 통해 최적의 액션 시퀀스를 산출합니다.</p></li>
<li><p>(추가적으로 MPC등이 도입될 수도 있을 것입니다.)</p></li>
</ol>
</div>
<ul class="simple">
<li><p>이러한 어프로치에서 우리는 리워드를 다음과 같이 정의할 수 있습니다.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
r(o_i, a_i) = -||g(o_i') - g(o_\text{goal})||^2
\]</div>
<div class="math notranslate nohighlight">
\[
r(s_i, a_i) = -||s'_i - s_\text{goal}||^2
\]</div>
<ul class="simple">
<li><p>주의할 점은, 임베딩된 스테이트가 정보를 잘 표현하고, 원래의 이미지로의 reconstruction도 잘 된다해도, <mark>반드시 state간의 L2 distance가 observation간의 유사도를 표현한다고 보장할 수는 없다</mark>는 것입니다.</p></li>
<li><p>따라서 이와 같이 L2-distnace를 기반으로 리워드를 정의할 경우, L2-distance가 스테이트 간의 유사도를 표현할 수 있도록 옵저베이션을 임베딩해야 합니다.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\begin{split}
s_i \leftarrow g_\text{enc}(o_i;~\theta_\text{enc}) \\
\hat o_i \leftarrow g_\text{dec}(s_i;~\theta_\text{dec}) \\\end{split}\\\begin{split}\theta_\text{enc} \leftarrow \arg\min_{\theta_\text{enc}} 
\sum_i||\hat o_i -o_i||^2 \\
\theta_\text{dec} \leftarrow \arg\min_{\theta_\text{dec}} 
\sum_i||\hat o_i -o_i||^2
\end{split}\end{aligned}\end{align} \]</div>
<ul class="simple">
<li><p>또한 우리는 <strong>Reconstruction Error</strong>를 고려해서, 모델뿐만이 아니라 옵저베이션을 임베딩하는 <mark><strong>인코더</strong>와 <strong>디코더</strong>도 피팅해야합니다.</mark></p></li>
<li><p>단순히 임베딩된 스테이트만을 사용해서 모델을 피팅한다면,</p></li>
<li><p>Encoder가 Trivial Solution —e.g., 모든 observation을 zero vector로 임베딩 — 을 산출할 수 있기 때문입니다.</p></li>
</ul>
</section>
<section id="id3">
<h2>참고 자료<a class="headerlink" href="#id3" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker, and Martin Riedmiller (<em>2015</em>). <a class="reference external" href="https://arxiv.org/abs/1506.07365">“Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images”</a>.</p></li>
<li><p>Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine, Pieter Abbeel (2015). <a class="reference external" href="https://arxiv.org/abs/1509.06113">“Deep Spatial Autoencoders for Visuomotor Learning”</a>.</p></li>
<li><p>Michael Janner (2019). <a class="reference external" href="https://bair.berkeley.edu/blog/2019/12/12/mbpo/">“Model-Based Reinforcement Learning: Theory and Practice”</a>.</p></li>
<li><p>Ruben Villegas, Arkanath Pathak, Harini Kannan, Dumitru Erhan, Quoc V. Le, and Honglak Lee (2019). <a class="reference external" href="https://arxiv.org/abs/1911.01655">“High Fidelity Video Prediction with Large Stochastic Recurrent Neural Networks”</a>.</p></li>
<li><p>Chelsea Finn, Sergey Levine (2016). <a class="reference external" href="https://arxiv.org/abs/1610.00696">“Deep Visual Foresight for Planning Robot Motion”</a></p></li>
<li><p>Chelsea Finn (2020). <a class="reference external" href="https://www.youtube.com/watch?v=LGhVQ3NB9h4&amp;list=PLoROMvodv4rOxuwpC_raecBCd5Jf54lEa&amp;t=2006s">“Stanford CS330:Multi-task and Meta Learning | 2020 | Lecture 10 - Model-Based Reinforcement Learning”</a> (YouTube).</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./_posts/multitask-meta-learning/Ch9 RL - Model-Based Reinforcement Learning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="../Ch9%20RL%20-%20A%20Primer%2C%20Multi-Task%2C%20Goal-Conditioned.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">CS330 Lec9. Reinforcemeht Learning : A Primer, Multi-Task, Goal-Conditioned</p>
        </div>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By 가짜연구소 멀티태스크/메타러닝 초읽기 팀<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>