
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>CS330 Lec11. Meta RL: Adaptable Models &amp; Policies &#8212; 멀티태스크/메타러닝 초읽기</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="CS330 Lec9. Reinforcemeht Learning : A Primer, Multi-Task, Goal-Conditioned" href="Ch9%20RL%20-%20A%20Primer%2C%20Multi-Task%2C%20Goal-Conditioned.html" />
    <link rel="prev" title="Lect 10 Model-Based RL" href="Ch10%20RL%20-%20Model-Based%20Reinforcement%20Learning/index.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/PseudoLab_logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">멀티태스크/메타러닝 초읽기</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    인트로
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Deep Multi-Task &amp; Meta Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Ch10%20RL%20-%20Model-Based%20Reinforcement%20Learning/index.html">
   Lect 10 Model-Based RL
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   CS330 Lec11. Meta RL: Adaptable Models &amp; Policies
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Ch9%20RL%20-%20A%20Primer%2C%20Multi-Task%2C%20Goal-Conditioned.html">
   CS330 Lec9. Reinforcemeht Learning : A Primer, Multi-Task, Goal-Conditioned
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/executablebooks/Deep-Multi-Task-and-Meta-Learning"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/executablebooks/Deep-Multi-Task-and-Meta-Learning/issues/new?title=Issue%20on%20page%20%2F_posts/multitask-meta-learning/Ch11 RL - Meta RL: Adaptable Models & Policies.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/_posts/multitask-meta-learning/Ch11 RL - Meta RL: Adaptable Models & Policies.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#meta-reinforcement-learning-problem-statement">
   Meta Reinforcement Learning Problem statement
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#remind">
     Remind
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#formulation">
     Formulation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example">
     Example
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#black-box-meta-rl-methods">
   Black box Meta RL methods
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#black-box-meta-rl">
     Black-box Meta-RL
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-1">
     Example 1
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-2">
     Example 2
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#digression-connection-to-multi-task-policies">
     Digression: Connection to Multi-Task Policies
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#summary">
     Summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimization-based-meta-rl-methods">
   Optimization based Meta RL methods
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#overview">
     Overview
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#maml-policy-gradients">
     MAML + Policy Gradients
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#maml-model-based-rl">
     MAML + Model-Based RL
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#questions">
   Questions
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>CS330 Lec11. Meta RL: Adaptable Models & Policies</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#meta-reinforcement-learning-problem-statement">
   Meta Reinforcement Learning Problem statement
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#remind">
     Remind
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#formulation">
     Formulation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example">
     Example
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#black-box-meta-rl-methods">
   Black box Meta RL methods
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#black-box-meta-rl">
     Black-box Meta-RL
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-1">
     Example 1
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-2">
     Example 2
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#digression-connection-to-multi-task-policies">
     Digression: Connection to Multi-Task Policies
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#summary">
     Summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimization-based-meta-rl-methods">
   Optimization based Meta RL methods
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#overview">
     Overview
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#maml-policy-gradients">
     MAML + Policy Gradients
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#maml-model-based-rl">
     MAML + Model-Based RL
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#questions">
   Questions
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="cs330-lec11-meta-rl-adaptable-models-policies">
<h1>CS330 Lec11. Meta RL: Adaptable Models &amp; Policies<a class="headerlink" href="#cs330-lec11-meta-rl-adaptable-models-policies" title="Permalink to this headline">#</a></h1>
<blockquote>
<div><p>Organization: 가짜연구소 (Pseudo Lab)<br />
Editor: <span class="xref myst">민예린 (Yerin Min)</span><br />
강의 자료: <a class="reference external" href="http://cs330.stanford.edu/fall2020/slides/cs330_metarl1_2020.pdf">CS330 2020 Fall</a><br />
강의 영상: <a class="reference external" href="https://youtu.be/TPNs9sNNQwc">Youtube</a></p>
</div></blockquote>
<p>안녕하세요, AgileSoDA에서 강화학습 연구원으로 재직중인 민예린 입니다. 5기에 이어 6기까지 빌더로 참여하고 있는데, 배울점이 많은 분들과 함께 스터디를 진행할 수 있어서 즐거운 마음으로 하고 있습니다.<br/>
아직 부족함이 많지만 열심히 준비하였으니, 너그러운 마음으로 중간에 오류가 있다면 언제든 말씀 부탁드리겠습니다!
<br/><br/></p>
<section id="meta-reinforcement-learning-problem-statement">
<h2>Meta Reinforcement Learning Problem statement<a class="headerlink" href="#meta-reinforcement-learning-problem-statement" title="Permalink to this headline">#</a></h2>
<section id="remind">
<h3>Remind<a class="headerlink" href="#remind" title="Permalink to this headline">#</a></h3>
<p>본 강의(CS330)의 1 ~ 8강에서는 강화학습에 집중되기 보다는 방법론에 대한 설명을 주로 하고 있습니다.
<img alt="slide 1" src="../../_images/lec11_1.png" /></p>
<p>본 내용에들어가기 앞서서 이전 내용을 되살려 보고자 합니다. Lec 11에서는 아래 내용들을 기반으로 Meta Reinforcement Learning(이하 Meta-RL)에 대해 설명하고 있습니다.</p>
<ul class="simple">
<li><p>Multi-Task Learning: 동시에 다양한 tasks를 학습하는 방법론</p></li>
<li><p>Transfer Learning: Source tasks를 학습한 후 Target task를 기존 지식 기반으로 학습하는 방법론</p></li>
<li><p>Meta Learning: 주어진 task dataset으로부터 다양한 tasks에 대해 학습한 후 새로운 task를 빠르게 학습(적용)하는 방법론
<br/><br/></p></li>
</ul>
</section>
<section id="formulation">
<h3>Formulation<a class="headerlink" href="#formulation" title="Permalink to this headline">#</a></h3>
<p>먼저 Supervised Learning(이하 SL)과 Meta Supervised Learning(이하 Meta-SL)을 비교해보겠습니다.
<img alt="slide 2" src="../../_images/lec11_2.png" />
SL은 input x에 대한 output y를 예측하는 방법론이고, 그렇기 때문에 data는 (x, y) 쌍으로 이루어져 있습니다. 흔하게 생각할 수 있는 개와 고양이 분류 등이 이에 해당합니다.<br/>
반면 Meta-SL의 경우엔 meta train에서 다양한 tasks를 학습해야 하기 때문에 input data <span class="math notranslate nohighlight">\(D_{train}\)</span>은 각 tasks에 대한 (x, y) 쌍의 data를 보유하고 있으며, new task에 대한 test data (x, y) 쌍도 보유하고 있습니다.<br/><br/></p>
<p><img alt="slide 3" src="../../_images/lec11_3.png" />
이제 위 내용을 RL formulation에 대입하여 보겠습니다.<br/>
RL의 경우 현재 state(<span class="math notranslate nohighlight">\(s_t\)</span>)에 대한 action(<span class="math notranslate nohighlight">\(a_t\)</span>)를 예측하는 방법론입니다. 그렇기 때문에 data는 현재의 state와 action, 그리고 action 대한 결과인 reward와 next state로 이루여져 있습니다. (<span class="math notranslate nohighlight">\(s_t, a_t, r_t, s_{t+1}\)</span>)<br/>
Meta-RL은 위 SL과 Meta-SL 관계와 유사합니다. Meta-RL에서는 각 task에 대한 dataset을 이용하여 train을 진행하고 최종적으로는 풀고자하는 new task(<span class="math notranslate nohighlight">\(s_t\)</span>)에 적용하게 됩니다.<br/><br/>
강의에서는 Meta-RL을 위해서는 아래 두 파트가 중요하다고 말하고 있습니다.</p>
<ul class="simple">
<li><p>k rollouts을 어떻게 모을 것인가?</p></li>
<li><p>each task에 대한 datasets을 어떻게 모을 것인가?</p></li>
</ul>
<p>먼저 k rollouts의 경우 풀고자하는 task를 풀기 위하여 exploration을 잘 해야 하는데, 이때 이 exploration을 얼마나 잘 할 수 있는지에 대한 learning problem과 관련이 있다고 설명하고 있습니다. 아마 task를 잘 배우기 위해서 k rollouts에서는 task를 배우기 위한 충분한 데이터를 가지고 있어야 하는데, 강화학습이 그렇듯 exploration을 잘 하면서 학습하기 위한 data가 있어야 한다고 말하는 것 같습니다.<br/>
그리고 each task에 대한 datasets은 다음 강의에서 설명해주신다고 하니, 다음 발표자분의 자료에서 확인할 수 있을 거 같습니다!</p>
<p>마지막으로 dataset에 대한 추가적인 설명을 하고 예시로 넘어가겠습니다.
<img alt="slide 4" src="../../_images/lec11_4.png" />
본 강의에서는 Meta-RL에서 사용하는 dataset이 문제 정의에 따라서 trajectory 기반의 episodic data일 수도 있고, timestep 기반의 data일 수도 있다고 합니다.
이는 강화학습에서 문제를 풀 때 이용하는 방식과 동일하게 생각하면 될 것 같습니다.
<br/><br/></p>
</section>
<section id="example">
<h3>Example<a class="headerlink" href="#example" title="Permalink to this headline">#</a></h3>
<p>강의에서는 Meta-RL을 쉽게 이해하기 위해서 Maze 문제에 빗대어 설명하고 있습니다. 이때 task를 navigating different maze라고 설명하는데, (start position, goal position)이 달라서 가야 하는 경로가 다른 경우를 서로 다른 task다 라고 말하고 있는 것 같습니다.<br/></p>
<p><img alt="slide 5" src="../../_images/lec11_5.png" />
Meta Learning은 train time에서 다양한 tasks를 학습한 후 new task에 빠르게 적용하기 위한 방법이기 때문에, train time에서 다양한 map 또는 다양한 (start, goal)을 학습하게 됩니다.</p>
<p><img alt="slide 6" src="../../_images/lec11_6.png" />
그리고 test time에서는 위 왼쪽 이미지(<span class="math notranslate nohighlight">\(D_{train}\)</span>)처럼 몇 번의 경험들을 저장한 data를 활용하여서 오른쪽 문제를 풀 수 있게 됩니다.<br/>
이때 train에서 exploration한 trajectories를 활용한다면 오른쪽 start point에서 goal point로 가는 경로를 찾을 수 있고 start point를 re-instantiate 하여 기존 경험을 활용할 수 있다고 합니다.<br/>
이런 경우를 강의에서는 few shot RL이라고 언급하여 넘어가고 있습니다.
<br/><br/></p>
</section>
</section>
<hr class="docutils" />
<section id="black-box-meta-rl-methods">
<h2>Black box Meta RL methods<a class="headerlink" href="#black-box-meta-rl-methods" title="Permalink to this headline">#</a></h2>
<section id="black-box-meta-rl">
<h3>Black-box Meta-RL<a class="headerlink" href="#black-box-meta-rl" title="Permalink to this headline">#</a></h3>
<p>Meta Learning은 크게 1) Black-box based, 2) Optimization based, non-parametric based로 구분될 수 있습니다. 이에 대한 내용은 CS330 1~8강에서 반복적으로 설명하고 있습니다.<br/></p>
<p><img alt="slide 7" src="../../_images/lec11_7.png" />
이전 강의에서 배웠던 Black-box meta learning의 경우 train dataset에서 dataset 내 순서가 없었으나, RL의 경우 current state, action, next state 등 데이터 사이의 시간적인 연결 관계가 있습니다.<br/>
따라서 일반적으로 data processing을 위하여 recurrent network를 사용하기도 한다고 합니다. current time step t 이전까지의 trajectories를 활용하여서 학습을 한다고 설명하고 있습니다.</p>
<p><img alt="slide 8" src="../../_images/lec11_8.png" />
여기서 생각해 볼 것은 기존 recurrent network를 활용한 RL과 지금의 recurrent network 기반 Meta-RL이 어떤 차이가 있는지에 대한 것입니다.
강의에서는 두 가지 차이를 설명하고 있습니다.</p>
<ul class="simple">
<li><p>먼저, reward가 input으로 들어가며 다양한 MDPs를 동시에 학습한다는 것입니다.</p></li>
<li><p>두 번째는, 동일한 task 내에서는 episode가 바뀌어도 hidden state를 계속 유지한다는 것에 차이가 있다고 합니다.</p></li>
</ul>
<p>단순하게 생각해보면 일반적인 RL에서는 input으로 observation들어가고 output으로 action이 나오게 되는데, 위 그림으로 봐도 meta-rl에서는 input으로 reward(이전 보상) 정보까지 활용하고 있는 것을 볼 수 있고, 동시에 다양한 tasks를 학습한다는 차이가 분명하게 존재하는 것을 알 수 있습니다.<br/></p>
<p><img alt="slide 9" src="../../_images/lec11_9.png" />
더 쉽게 이해하기 위해 학습 프로세스를 보겠습니다.Black-box Meta-RL은 위와 같은 프로세스로 학습을 진행하며, meta test time에는 1, 2번만 진행합니다.</p>
<ul class="simple">
<li><p>roll-out : 쉽게 생각하자면 trajecory라고 이해하면 된다고 합니다. 추가적으로 설명을 붙여보자면 policy를 통해서 state-&gt;action을 얻는 과정을 rolling out 한다고 하는 것 같습니다. policy에서 input으로부터 ouput을 얻는 과정들을 통해 얻게 되는 trajectories를 의미한다고 이해하면 될 거 같습니다.</p></li>
</ul>
<p>따라서 일반적인 RL 학습 과정에서 task sampling 하는 부분이 추가된다고 이해하면 쉽습니다.</p>
<ol class="simple">
<li><p>task를 sampling 하여 가져온 후</p></li>
<li><p>policy를 통해 trajectories를 수집</p></li>
<li><p>buffer에 저장</p></li>
<li><p>모든 tasks에 대하여 reward를 최대화 하기 위한 policy update</p></li>
</ol>
<p>아래는 강의에서 예시로 보여줬던 Architectures &amp; Optimizers for Black-box Meta-RL 인데, 참고 이미지로 한 번 보셔도 좋을 거 같습니다!
<img alt="slide 10" src="../../_images/lec11_10.png" />
<br/><br/></p>
</section>
<section id="example-1">
<h3>Example 1<a class="headerlink" href="#example-1" title="Permalink to this headline">#</a></h3>
<p>위 Black-box Meta RL의 첫 예시로 <a class="reference external" href="https://arxiv.org/pdf/1707.03141.pdf">A Simple Neural Attentive Meta-Learner. ICLR 2018</a> 논문을 간단하게 소개하고 있습니다.<br/></p>
<p><img alt="slide 11" src="../../_images/lec11_11.png" />
위 논문에서 실험한 예를 살펴보면 1000개의 mazes에서 training을 진행한 후 test mazes에서 길을 찾는 과정을 볼 수 있습니다.</p>
<p><img alt="slide 12" src="../../_images/lec11_12.png" />
Finn 교수님께서 본 논문의 contribution으로 강조하는 부분은 test time의 Episode 1에서 충분한 maze 탐색을 진행한 것 만으로 Episode 2에서는 바로 goal path를 찾는다는 것이었습니다.
///
<br/><br/></p>
</section>
<section id="example-2">
<h3>Example 2<a class="headerlink" href="#example-2" title="Permalink to this headline">#</a></h3>
<p>두 번째 예시는 <a class="reference external" href="https://openreview.net/pdf?id=BJeMeiCVd4">Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables. ICML 2019</a>입니다.<br/>
예시들을 본 강의에서는 간단하게 설명하고 넘어가고 있는데, 본 논문은 meta-rl이 continuous control problems에서도 좋은 성능을 내는 걸 확인할 수 있는 논문이라고 합니다.<br/>
<img alt="slide 13" src="../../_images/lec11_13.png" /></p>
<p>논문에서 좋은 성능을 얻었다고 말함에도 불구하고 Finn 교수님께서는 Meta-RL이 test time에서는 꽤 효과적인 방법일 수 있지만, train time에서는 효과적인 학습 방법이 아닐 수 있다고 말씀하셨습니다.<br/>
그리고 여러 tasks를 모두 배우기 위해서는 굉장히 많은 데이터가 필요하다고 합니다.<br/>
여기서 생각해볼 만한 질문을 하나 던져주셨습니다.<br/></p>
<ul class="simple">
<li><p>질문 : Off-policy meta-rl과 on-policy meta-rl 중 더 efficient 한 학습 방법은 무엇인가?</p></li>
<li><p>답변 : Off-policy meta-rl. Off-policy meta-rl은 학습 과정 중에 나온 trajectories를 replay buffer에 저장하고, 이를 sampling하여 학습하기 때문에 data efficiency가 좋습니다. 따라서 많은 양의 데이터가 필요한 meta-rl의 경우 on-policy 보다는 off-policy가 더 효율적인 방법이라고 할 수 있습니다.
<br/><br/></p></li>
</ul>
</section>
<section id="digression-connection-to-multi-task-policies">
<h3>Digression: Connection to Multi-Task Policies<a class="headerlink" href="#digression-connection-to-multi-task-policies" title="Permalink to this headline">#</a></h3>
<p>강의 중간에 첨언 같은 성격으로 Multi-task policy와의 connection에 대해 갑자기 설명하고 있습니다! Meta-rl도 어떻게 보면 multi-task rl의 관점에서 문제를 바라볼 수 있다고 하는 거 같습니다.<br/></p>
<p><img alt="slide 14" src="../../_images/lec11_14.png" />
Experience가 task identifier로서 어떤 task에 해당하는 것인지 알 수 있는 정보로 이용될 수 있다고 합니다.</p>
<p><img alt="slide 15" src="../../_images/lec11_15.png" />
그리고 추가적으로 goal-conditioned에 대해서도 meta-rl과 연관지어서 설명해주셨습니다.<br/>
Goal-conditioned의 경우 task identifier는 different goal에 해당하며, meta-rl의 경우 goal-conditioned가 strict generalization 하다고 합니다.이는 rewards가 goals, 즉 other task로 표현될 수 있기 때문이라고 합니다.<br/>
생각해봤을 때 reward가 <span class="math notranslate nohighlight">\(|s_t - s_{goal}|\)</span> 이라면 goal이 달라지면 자동적으로 reward가 달라지기 때문에 strict generalization이라고 표현하는 것 같습니다.
<br/><br/></p>
</section>
<section id="summary">
<h3>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">#</a></h3>
<p>정리하자면, black-box meta-rl은 아래와 같은 특징을 가지고 있습니다.</p>
<ul class="simple">
<li><p>general &amp; expressive</p></li>
<li><p>a variety of degign choices in architecture</p></li>
<li><p>hard to optimize</p></li>
<li><p>inherits sample efficiency from outer RL optimizer
<br/><br/></p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="optimization-based-meta-rl-methods">
<h2>Optimization based Meta RL methods<a class="headerlink" href="#optimization-based-meta-rl-methods" title="Permalink to this headline">#</a></h2>
<section id="overview">
<h3>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">#</a></h3>
<p>다음은 Optimization based Meta-RL 입니다.<br/></p>
<p><img alt="slide 16" src="../../_images/lec11_16.png" />
기존 optimization based meta learning에서, SL dataset을 활용하여 inner loop을 학습했다면, Meta-RL에서는 k rollouts를 활용하여 inner loop을 학습하게 됩니다.<br/></p>
<p>이때 생각해볼 만한 질문을 하나 제시해 주셨습니다.<br/></p>
<ul class="simple">
<li><p>질문 : Inner optimization에서 이용해야 하는 method와 그 이유는 무엇인가?</p></li>
<li><p>답변 : Model-based RL을 추천한다고 합니다. 먼저, 그 이유는 off-policy method이기 때문에 data efficiency하고 gradient-based method라는 장점이 있기 때문이라고 하셨습니다. 또한 optimization based meta-rl과 combine이 용이하기 때문에 model-based rl이 가장 추천하는 방법이라고 합니다.
<img alt="slide 17" src="../../_images/lec11_17.png" />
<br/><br/></p></li>
</ul>
</section>
<section id="maml-policy-gradients">
<h3>MAML + Policy Gradients<a class="headerlink" href="#maml-policy-gradients" title="Permalink to this headline">#</a></h3>
<p>위 질문과 연계로 MAML(optimization based meta learning) + policy gradients에 대한 예시를 간단하게 설명해주셨습니다.<br/>
<a class="reference external" href="https://arxiv.org/pdf/1703.03400.pdf">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. ICML 2017</a> 즉 MAML 논문은 대표적인 optimization based meta learning 논문입니다.<br/></p>
<p><img alt="slide 18" src="../../_images/lec11_18.png" />
위 논문에 있는 예시로 &lt;-, -&gt; 각 방향 direction을 가지는 2 tasks가 있을 때 inner optimization이 진행되면서 tasks 능숙도가 발전되어 가는 것을 볼 수 있습니다.<br/>
강의에서 교수님이 간단하게만 설명하고 넘어가셨는데 0 gradient step과 1 gradient step의 움직임을 비교해 봤을 때 확연한 차이가 있었습니다.
<br/><br/></p>
</section>
<section id="maml-model-based-rl">
<h3>MAML + Model-Based RL<a class="headerlink" href="#maml-model-based-rl" title="Permalink to this headline">#</a></h3>
<p>다음은 MAML + Model-Based RL에 대한 예시입니다. Model-based RL은 Lec10에서 배웠던 것과 같이 환경으로 사용 가능한 transition model을 활용하여 next state와 reward를 구하는 강화학습 방법입니다.</p>
<p><img alt="slide 19" src="../../_images/lec11_19.png" />
<a class="reference external" href="https://arxiv.org/pdf/1803.11347.pdf">Learning to Adapt in Dynamic Environments through Meta-RL. ICLR ‘19</a> 논문의 예시에서는 time steps 기준으로 dataset을 구성하고 있습니다.<br/></p>
<p><img alt="slide 20" src="../../_images/lec11_20.png" />
Meta training은 optimization based meta-rl에서 inner optimization에 해당하며, <span class="math notranslate nohighlight">\(D_{tr}\)</span>으로 <span class="math notranslate nohighlight">\(\theta\)</span>를 학습하는 것을 의미힙니다.<br/>
이때 time step을 기준으로 task를 구분해 학습했다고 합니다. 이때 구분된 task 내에서도 <span class="math notranslate nohighlight">\(D_{tr}\)</span> <span class="math notranslate nohighlight">\(D_{ts}\)</span>로 분리하여 dataset을 구성, 학습한 것으로 보입니다.</p>
<p><img alt="slide 21" src="../../_images/lec11_21.png" />
Meta testing은 outer optimization에 해당되며, meta training에서 학습된 <span class="math notranslate nohighlight">\(\theta\)</span>를 활용하여 meta parameter인 <span class="math notranslate nohighlight">\(\phi\)</span>를 update하게 됩니다.<br/></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f_{\theta} \rightarrow f_{\phi_t}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\phi_t \leftarrow \theta - \alpha \nabla_{\theta}L(\theta, \theta^{tr}_t\)</span>
<br/><br/></p></li>
</ul>
</section>
<section id="id1">
<h3>Summary<a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h3>
<p>정리하자면, optimization based meta-rl과 black-box based meta-rl은 각각 아래 이미지와 같은 특징들을 가진다고 합니다.
<img alt="slide 22" src="../../_images/lec11_22.png" /></p>
<p>기존 meta learning에서 가졌던 특징들을 그대로 이어서 보유하고 있는 것 같습니다.
<br/><br/></p>
</section>
</section>
<hr class="docutils" />
<section id="questions">
<h2>Questions<a class="headerlink" href="#questions" title="Permalink to this headline">#</a></h2>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./_posts/multitask-meta-learning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="Ch10%20RL%20-%20Model-Based%20Reinforcement%20Learning/index.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Lect 10 Model-Based RL</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="Ch9%20RL%20-%20A%20Primer%2C%20Multi-Task%2C%20Goal-Conditioned.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">CS330 Lec9. Reinforcemeht Learning : A Primer, Multi-Task, Goal-Conditioned</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By 가짜연구소 멀티태스크/메타러닝 초읽기 팀<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>